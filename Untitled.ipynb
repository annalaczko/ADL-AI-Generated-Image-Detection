{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dd787f-d1d4-44a2-9f77-3dc554a3bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Custom Dataset to load images and labels from filenames\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Extract label from the filename (fake -> 1, real -> 0)\n",
    "        label = 1 if \"fake\" in os.path.basename(img_path).lower() else 0\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# Define transformations for training and testing\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((512,512)), #transforms.Resize((256,256)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize the images\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((512,512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f692af73-4581-40b2-a3ca-210849db7ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets using CustomImageDataset\n",
    "train_data = CustomImageDataset(directory='data/processed/train', transform=train_transforms)\n",
    "test_data = CustomImageDataset(directory='data/processed/test', transform=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16001797-53dc-40d3-b5e6-f08253cdd13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AIImageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AIImageClassifier, self).__init__()\n",
    "        self.input_size = (512,512)\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Input: 3xHxW, Output: 32xHxW\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "#        self.pool1 = nn.MaxPool2d(2, 2)  # Downsample: HxW -> H/2 x W/2\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Output: 64xH/2xW/2\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(4, 4)  # Downsample: H/2xW/2 -> H/4xW/4\n",
    "\n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # Output: 128xH/4xW/4\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(4, 4)  # Downsample: H/4xW/4 -> H/8xW/8\n",
    "\n",
    "        # Fourth convolutional block\n",
    "        # self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)  # Output: 256xH/8xW/8\n",
    "        # self.bn4 = nn.BatchNorm2d(256)\n",
    "        # self.pool4 = nn.MaxPool2d(2, 2)  # Downsample: H/8xW/8 -> H/16xW/16\n",
    "\n",
    "        # Calculate the flattened size after convolutions\n",
    "        self._flattened_size = self._compute_flattened_size(self.input_size)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._flattened_size, 512)\n",
    "        # self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 2)  # Change input size here to 128 instead of 512\n",
    "        # self.dropout2 = nn.Dropout(0.5)\n",
    "        # self.fc3 = nn.Linear(128, 2)  # Binary classification (AI-generated or not)\n",
    "\n",
    "    def _compute_flattened_size(self, input_size):\n",
    "        \"\"\"Compute the size of the tensor after all convolutional and pooling layers.\"\"\"\n",
    "        x = torch.zeros(1, 3, *input_size)\n",
    "        #print(f\"Initial size: {x.size()}\")\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        #print(f\"After pool1: {x.size()}\")\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        #print(f\"After pool2: {x.size()}\")\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        #print(f\"After pool3: {x.size()}\")\n",
    "        # x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
    "        # #print(f\"After pool4: {x.size()}\")\n",
    "        return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        # x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
    "\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x = self.dropout1(x)\n",
    "        x = self.fc2(x) # Now correctly sized input\n",
    "        # x = self.dropout2(x)\n",
    "        # x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d947213-65a7-45d0-b715-9d406a196690",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84484b91-0a49-48fd-95a0-04a05e12f805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "total_bytes = sum(torch.cuda.memory_stats().values())\n",
    "total_mbs = total_bytes / (1024 * 1024)\n",
    "print(f\"Total size: {total_mbs:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b85e679-291b-4fb7-bce1-2cccab5d385a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Epoch 1/5, Loss: 1705.2456572532653\n",
      "Epoch 2/5, Loss: 14.530432891845702\n",
      "Epoch 3/5, Loss: 2.54506476521492\n",
      "Epoch 4/5, Loss: 1.4967406451702119\n",
      "Epoch 5/5, Loss: 0.7806993722915649\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = AIImageClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "046db8b1-57a2-447f-939e-5376a01c27d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 51.088201603665524%\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7031b0-3545-491a-ba93-0d9a820537de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32f4af-a196-4ec2-b784-aa18ba6f47b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ADL with GPU",
   "language": "python",
   "name": "adl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
